{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fac191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn import tree\n",
    "from collections import Counter\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split, cross_val_score, KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, make_scorer, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean, std\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pathlib import Path\n",
    "import xgboost \n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils import resample\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "%matplotlib inline\n",
    "\n",
    "from graphviz import Source\n",
    "import pydot\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c748e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_names = ['class drop', 'class stay']\n",
    "\n",
    "precision_stay_final = []\n",
    "precision_drop_final = []\n",
    "recall_stay_final = []\n",
    "recall_drop_final = []\n",
    "recall_general_final = []\n",
    "accuracy_final = []\n",
    "class_imbalance_final = []\n",
    "clf_list = []\n",
    "\n",
    "def add_to_final_table(X_train, y_train, clf, names):\n",
    "   \n",
    "    y_pred = cross_val_predict(estimator=clf, X=X_train, y=y_train, cv=5)\n",
    "    #y_pred = clf.predict(X_test)\n",
    "    eval_df = classification_report(y_train, y_pred, digits=3, target_names=names, output_dict=True)\n",
    "    eval_df = pd.DataFrame(eval_df).transpose()\n",
    "    \n",
    "    precision_stay = eval_df['precision'][1]\n",
    "    precision_drop = eval_df['precision'][0]\n",
    "    recall_stay = eval_df['recall'][1]\n",
    "    recall_drop = eval_df['recall'][0]\n",
    "    recall_general = eval_df['recall'][3]\n",
    "    accuracy = eval_df['precision'][2]\n",
    "    class_imbalance = eval_df['support'][0] / eval_df['support'][1]\n",
    "    precision_stay_final.append(precision_stay)\n",
    "    precision_drop_final.append(precision_drop)\n",
    "    recall_stay_final.append(recall_stay)\n",
    "    recall_drop_final.append(recall_drop)\n",
    "    recall_general_final.append(recall_general)\n",
    "    accuracy_final.append(accuracy)\n",
    "    class_imbalance_final.append(class_imbalance)\n",
    "    clf_list.append(clf)\n",
    "    \n",
    "def XGBoost_to_final_table(y_pred, y_train, names):\n",
    "    eval_df = classification_report(y_train, y_pred, digits=3, target_names=names, output_dict=True)\n",
    "    eval_df = pd.DataFrame(eval_df).transpose()\n",
    "    \n",
    "    precision_stay = eval_df['precision'][1]\n",
    "    precision_drop = eval_df['precision'][0]\n",
    "    recall_stay = eval_df['recall'][1]\n",
    "    recall_drop = eval_df['recall'][0]\n",
    "    recall_general = eval_df['recall'][3]\n",
    "    accuracy = eval_df['precision'][2]\n",
    "    class_imbalance = eval_df['support'][0] / eval_df['support'][1]\n",
    "    precision_stay_final.append(precision_stay)\n",
    "    precision_drop_final.append(precision_drop)\n",
    "    recall_stay_final.append(recall_stay)\n",
    "    recall_drop_final.append(recall_drop)\n",
    "    recall_general_final.append(recall_general)\n",
    "    accuracy_final.append(accuracy)\n",
    "    class_imbalance_final.append(class_imbalance)\n",
    "    clf_list.append('XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45da4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking on test set\n",
    "precision_stay_test = []\n",
    "precision_drop_test = []\n",
    "recall_stay_test = []\n",
    "recall_drop_test = []\n",
    "recall_general_test = []\n",
    "accuracy_test = []\n",
    "class_imbalance_test = []\n",
    "clf_test = []\n",
    "\n",
    "def add_to_final_table_test(X_test, y_test, clf, names):\n",
    "    #y_pred = cross_val_predict(estimator=clf, X=X_train, y=y_train, cv=5)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    eval_df = classification_report(y_test, y_pred, digits=3, target_names=names, output_dict=True)\n",
    "    eval_df = pd.DataFrame(eval_df).transpose()\n",
    "    \n",
    "    precision_stay = eval_df['precision'][1]\n",
    "    precision_drop = eval_df['precision'][0]\n",
    "    recall_stay = eval_df['recall'][1]\n",
    "    recall_drop = eval_df['recall'][0]\n",
    "    recall_general = eval_df['recall'][3]\n",
    "    accuracy = eval_df['precision'][2]\n",
    "    class_imbalance = eval_df['support'][0] / eval_df['support'][1]\n",
    "    precision_stay_test.append(precision_stay)\n",
    "    precision_drop_test.append(precision_drop)\n",
    "    recall_stay_test.append(recall_stay)\n",
    "    recall_drop_test.append(recall_drop)\n",
    "    recall_general_test.append(recall_general)\n",
    "    accuracy_test.append(accuracy)\n",
    "    class_imbalance_test.append(class_imbalance)\n",
    "    clf_test.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c749b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutting unnessesary leaves\n",
    "# https://stackoverflow.com/questions/51397109/prune-unnecessary-leaves-in-sklearn-decisiontreeclassifier\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "def is_leaf(inner_tree, index):\n",
    "    # Check whether node is leaf node\n",
    "    return (inner_tree.children_left[index] == TREE_LEAF and \n",
    "            inner_tree.children_right[index] == TREE_LEAF)\n",
    "\n",
    "def prune_index(inner_tree, decisions, index=0):\n",
    "    # Start pruning from the bottom - if we start from the top, we might miss\n",
    "    # nodes that become leaves during pruning.\n",
    "    # Do not use this directly - use prune_duplicate_leaves instead.\n",
    "    if not is_leaf(inner_tree, inner_tree.children_left[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_left[index])\n",
    "    if not is_leaf(inner_tree, inner_tree.children_right[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_right[index])\n",
    "\n",
    "    # Prune children if both children are leaves now and make the same decision:     \n",
    "    if (is_leaf(inner_tree, inner_tree.children_left[index]) and\n",
    "        is_leaf(inner_tree, inner_tree.children_right[index]) and\n",
    "        (decisions[index] == decisions[inner_tree.children_left[index]]) and \n",
    "        (decisions[index] == decisions[inner_tree.children_right[index]])):\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "        ##print(\"Pruned {}\".format(index))\n",
    "\n",
    "def prune_duplicate_leaves(mdl):\n",
    "    # Remove leaves if both \n",
    "    decisions = mdl.tree_.value.argmax(axis=2).flatten().tolist() # Decision for each node\n",
    "    prune_index(mdl.tree_, decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f50c8",
   "metadata": {},
   "source": [
    "defining all the models with default and tuned parameters, adding results to final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d80394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# DECISION TREE #########################################\n",
    "#########################################################\n",
    "\"\"\"   \n",
    "# trying to reduce complexity (cost complexity pruning) and find optimal parameters using CV:\n",
    "def decision_tree_ccp_alpha(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    clf = DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "    ccp_alphas = path['ccp_alphas']\n",
    "    alpha_loop_values = []\n",
    "    for alpha in ccp_alphas[-20:]:\n",
    "        clf_dt = DecisionTreeClassifier(ccp_alpha=alpha)\n",
    "        scores = cross_val_score(clf_dt, X_train, y_train, cv=5)\n",
    "        alpha_loop_values.append([alpha, np.mean(scores), np.std(scores)])\n",
    "\n",
    "    alpha_results = pd.DataFrame(alpha_loop_values, columns=['alpha', 'mean_accuracy', 'std'])\n",
    "    #alpha_results.plot(x='alpha', y='mean_accuracy', yerr='std', marker='o', figsize=(15, 4))\n",
    "    alpha_results = alpha_results.sort_values(by=['mean_accuracy'], ascending=False)\n",
    "    max_result = alpha_results[alpha_results['mean_accuracy']==alpha_results['mean_accuracy'].max()]['alpha']\n",
    "    opt_alpha = max_result.iloc[0]\n",
    "    clf = DecisionTreeClassifier(ccp_alpha=opt_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    add_to_final_table(X_train, y_train, clf)\n",
    "    add_to_final_table_test(X_test, y_test, clf)\n",
    "    \n",
    "\"\"\"\n",
    "    \n",
    "# DT    \n",
    "def decision_tree_250(X_train, y_train, X_test, y_test, names):\n",
    "    tree_params = {'min_samples_leaf': [20, 50, 100, 250]}\n",
    "    clf = GridSearchCV(DecisionTreeClassifier(max_depth=4, class_weight=\"balanced\"), tree_params, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_estimator_)\n",
    "    #clf = DecisionTreeClassifier(clf.best_estimator_)\n",
    "    #clf.fit(X_train, y_train)\n",
    "    prune_duplicate_leaves(clf.best_estimator_)\n",
    "    \n",
    "    add_to_final_table(X_train, y_train, clf, names)\n",
    "    add_to_final_table_test(X_test, y_test, clf, names)\n",
    "    #print('DecisionTreeClassifier')\n",
    "    print(' ')\n",
    "    \n",
    "    X_train.feature_names = X_train.columns\n",
    "    plt.figure(figsize=(45, 10))\n",
    "    _ = tree.plot_tree(clf.best_estimator_, filled=True, feature_names=X_train.feature_names, fontsize=12, precision=7, class_names=names)\n",
    "    plt.show()\n",
    "    \n",
    "                \n",
    "#########################################################\n",
    "# SVM (LinearSVC) #######################################\n",
    "#########################################################\n",
    "    \n",
    "# default parameters \n",
    "def SVM_default(X_train, y_train, X_test, y_test, names):\n",
    "        \n",
    "    clf = LinearSVC(max_iter=10000, class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    add_to_final_table(X_train, y_train, clf, names)\n",
    "    add_to_final_table_test(X_test, y_test, clf, names)\n",
    "    \n",
    "# C=0.01\n",
    "def SVM_C(X_train, y_train, X_test, y_test, names):\n",
    "    \n",
    "    clf = LinearSVC(C=0.01, max_iter=100000, class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    add_to_final_table(X_train, y_train, clf, names)\n",
    "    add_to_final_table_test(X_test, y_test, clf, names)\n",
    "    \n",
    "#########################################################\n",
    "# RBF SVM ###############################################\n",
    "#########################################################\n",
    "    \n",
    "# default parameters    \n",
    "def RBF_SVM(X_train, y_train, X_test, y_test, names):\n",
    "    \n",
    "    clf = SVC(class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    add_to_final_table(X_train, y_train, clf, names)\n",
    "    add_to_final_table_test(X_test, y_test, clf, names)\n",
    "    \n",
    "#########################################################\n",
    "# LOGISTIC REGRESSION ###################################\n",
    "#########################################################\n",
    "    \n",
    "# default parameters    \n",
    "def logistic_regression(X_train, y_train, X_test, y_test, names):\n",
    "    \n",
    "    clf = LogisticRegressionCV(cv=5, max_iter=10000, class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    add_to_final_table(X_train, y_train, clf, names)\n",
    "    add_to_final_table_test(X_test, y_test, clf, names)\n",
    "    print('The most important features and its coefficients obtained by logistic regression:')\n",
    "    feature_coef = pd.DataFrame(clf.coef_)\n",
    "    feature_coef.columns = X_train.columns\n",
    "    feature_coef.index = ['coef']\n",
    "    feature_coef = feature_coef.T\n",
    "    feature_coef_abs = abs(feature_coef['coef'])\n",
    "    feature_coef_abs = feature_coef_abs.sort_values(ascending=False)\n",
    "    feature_coef_abs_20 = feature_coef_abs[:20]\n",
    "    most_important_features = feature_coef_abs_20.index\n",
    "    #most_important_features_coeffs = feature_coef.loc[most_important_features]\n",
    "    \n",
    "    features = []\n",
    "    coeffs = []\n",
    "    #quality_features = df.filter(like='quality', axis=1).columns\n",
    "    knowledge_features = X_train.loc[:, X_train.columns.isin(['voting_age_awareness_w1',\n",
    "                                                    'KNOWLEDGE_PARLIAMENTARY_THRESHOLD_w1',\n",
    "                                                    'know_politicians_ratio',\n",
    "                                                    'whether_dropped_before',\n",
    "                                                    'lr_placement_correct'])].columns\n",
    "    joined_list = [*most_important_features, *knowledge_features] #*quality_features,\n",
    "    for i in joined_list:\n",
    "        coeff = feature_coef.T[i][0].round(3)\n",
    "        print(i, ': ', coeff)\n",
    "\n",
    "        \n",
    "#########################################################\n",
    "# XGBOOST ###############################################\n",
    "#########################################################\n",
    "\n",
    "# default parameters\n",
    "def XGBoost_default(X_train, y_train, X_test, y_test, names):    \n",
    "        \n",
    "    model = xgboost.XGBClassifier(verbosity=0, scale_pos_weight=Counter(y_train)[0]/Counter(y_train)[1])\n",
    "    y_pred = cross_val_predict(estimator=model, X=X_train, y=y_train, cv=5)\n",
    "    XGBoost_to_final_table(y_pred, y_train, names)\n",
    "\n",
    "# randomized search\n",
    "def XGBoost_RS(X_train, y_train, X_test, y_test, names):\n",
    "    model = xgboost.XGBClassifier(verbosity=0, scale_pos_weight=Counter(y_train)[0]/Counter(y_train)[1])\n",
    "    param = {\n",
    "     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30] ,\n",
    "     \"max_depth\"        : [3, 4, 5, 6, 10, 50],\n",
    "     \"min_child_weight\" : [1, 3, 5, 7],\n",
    "     \"gamma\"            : [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "     \"colsample_bytree\" : [0.3, 0.4, 0.5, 0.7],\n",
    "     'verbosity': [0]\n",
    "     }\n",
    "    XGB_random = RandomizedSearchCV(estimator = model, \n",
    "                           param_distributions = param, n_iter = 50, \n",
    "                           cv = 3)\n",
    "    XGB_random.fit(X_train, y_train)\n",
    "    y_pred = cross_val_predict(estimator=XGB_random, X=X_train, y=y_train, cv=5)\n",
    "    XGBoost_to_final_table(y_pred, y_train, names)\n",
    "    \n",
    "#########################################################\n",
    "# RANDOM FOREST #########################################\n",
    "#########################################################\n",
    "\n",
    "# default parameters\n",
    "def RF_default(X_train, y_train, X_test, y_test, names):    \n",
    "    \n",
    "    clf = RandomForestClassifier(class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    add_to_final_table(X_train, y_train, clf, names)\n",
    "    \"\"\"\n",
    "    sorted_idx = clf.feature_importances_.argsort()[:10]\n",
    "    plt.barh(X_train.columns[sorted_idx], clf.feature_importances_[sorted_idx])\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    \n",
    "    importances = clf.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "    std = np.std([\n",
    "        tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances[:10].plot.bar(yerr=std[:10], ax=ax)\n",
    "    ax.set_title(\"Feature importance - MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity (MDI)\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    result = permutation_importance(\n",
    "        clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    forest_importances = pd.Series(result.importances_mean, index=X_train.columns)\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances[:10].plot.bar(yerr=result.importances_std[:10], ax=ax)\n",
    "    ax.set_title(\"Feature importance after permutation\")\n",
    "    ax.set_ylabel(\"Decrease of error\")\n",
    "    fig.tight_layout()\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "# randomized search\n",
    "def RF_random_search(X_train, y_train, X_test, y_test, names): \n",
    "    \n",
    "    clf = RandomForestClassifier(class_weight=\"balanced\")\n",
    "    random_grid = {'bootstrap': [True, False],\n",
    "     'max_depth': [50, 100, 200],\n",
    "     'min_samples_leaf': [5, 10, 20],\n",
    "     'min_samples_split': [20, 40],\n",
    "     'n_estimators': [500, 2000, 4000]}\n",
    "    clf = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('Random_forest_best: ', clf.best_params_)\n",
    "    add_to_final_table(X_train, y_train, clf, names)\n",
    "    \"\"\"\n",
    "    sorted_idx = clf.feature_importances_.argsort()[:10]\n",
    "    plt.barh(X_train.columns[sorted_idx], clf.feature_importances_[sorted_idx])\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    \n",
    "    importances = clf.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "    std = np.std([\n",
    "        tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances[:10].plot.bar(yerr=std[:10], ax=ax)\n",
    "    ax.set_title(\"Feature importance - MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity (MDI)\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    \n",
    "    result = permutation_importance(\n",
    "        clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    forest_importances = pd.Series(result.importances_mean, index=X_train.columns)\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances[:10].plot.bar(yerr=result.importances_std[:10], ax=ax)\n",
    "    ax.set_title(\"Feature importance after permutation\")\n",
    "    ax.set_ylabel(\"Decrease of error\")\n",
    "    fig.tight_layout()\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.show()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2093faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(df, political_data):\n",
    "    personal_data = df.drop(['panelpat'], axis=1)\n",
    "    df = pd.concat([personal_data, political_data], axis=1)\n",
    "    return df\n",
    "\n",
    "def scale_train(X_train, X_test):\n",
    "    cols = X_train.columns\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train.columns = cols\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test.columns = cols\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15bd23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_all_models(X_train, y_train, X_test, y_test, names):\n",
    "    XGBoost_RS(X_train, y_train, X_test, y_test, names)\n",
    "    XGBoost_default(X_train, y_train, X_test, y_test, names)\n",
    "    RF_default(X_train, y_train, X_test, y_test, names)\n",
    "    RF_random_search(X_train, y_train, X_test, y_test, names)\n",
    "   # decision_tree_ccp_alpha(X_train, y_train, X_test, y_test)\n",
    "    #decision_tree_250(X_train, y_train, X_test, y_test, names)\n",
    "    \n",
    "    #X_train, X_test = scale_train(X_train, X_test)\n",
    "    #SVM_default(X_train, y_train, X_test, y_test, names)\n",
    "    #SVM_C(X_train, y_train, X_test, y_test, names)\n",
    "    #RBF_SVM(X_train, y_train, X_test, y_test, names)\n",
    "    #logistic_regression(X_train, y_train, X_test, y_test, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f19070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Analysis_functions_ensemble.ipynb to script\n",
      "[NbConvertApp] Writing 17710 bytes to Analysis_functions_ensemble.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Analysis_functions_ensemble.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e92c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
