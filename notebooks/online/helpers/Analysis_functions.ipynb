{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583c55dc-100f-4194-8320-5f8bd6665ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from graphviz import Source\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display_html\n",
    "from sklearn import tree\n",
    "from collections import Counter\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split, cross_val_score, KFold, cross_validate, GridSearchCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import cohen_kappa_score, roc_auc_score, confusion_matrix, precision_recall_curve, make_scorer, accuracy_score, precision_score, recall_score, f1_score, classification_report #brier_score_loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean, std\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pathlib import Path\n",
    "import xgboost\n",
    "from sklearn.utils import resample\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c748e7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\ndef XGBoost_to_final_table(y_pred, y_train):\\n    eval_df = classification_report(y_train, y_pred, digits=3, target_names=names, output_dict=True)\\n    eval_df = pd.DataFrame(eval_df).transpose()\\n    \\n    precision_stay = eval_df['precision'][1]\\n    precision_drop = eval_df['precision'][0]\\n    recall_stay = eval_df['recall'][1]\\n    recall_drop = eval_df['recall'][0]\\n    recall_general = eval_df['recall'][3]\\n    accuracy = eval_df['precision'][2]\\n    class_imbalance = eval_df['support'][0] / eval_df['support'][1]\\n    precision_stay_final.append(precision_stay)\\n    precision_drop_final.append(precision_drop)\\n    recall_stay_final.append(recall_stay)\\n    recall_drop_final.append(recall_drop)\\n    recall_general_final.append(recall_general)\\n    accuracy_final.append(accuracy)\\n    class_imbalance_final.append(class_imbalance)\\n    algorithm_list.append('XGBoost')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#target_names = ['class drop', 'class stay']\n",
    "\n",
    "precision_stay_train = []\n",
    "precision_drop_train = []\n",
    "recall_stay_train = []\n",
    "recall_drop_train = []\n",
    "recall_general_train = []\n",
    "accuracy_train = []\n",
    "rocauc_train = []\n",
    "cohen_kappa_train = []\n",
    "#brier_loss_train = []\n",
    "class_imbalance_train = []\n",
    "#clf_list = []\n",
    "algorithm_train = []\n",
    "\n",
    "def add_to_final_table(X_train, y_train, algorithm, clf, names):\n",
    "   \n",
    "    y_pred = cross_val_predict(estimator=clf, X=X_train, y=y_train)\n",
    "    #y_pred = clf.predict(X_test)\n",
    "    eval_df = classification_report(y_train, y_pred, digits=3, target_names=names, output_dict=True)\n",
    "    eval_df = pd.DataFrame(eval_df).transpose()\n",
    "    \n",
    "    precision_stay = eval_df['precision'][1]\n",
    "    precision_drop = eval_df['precision'][0]\n",
    "    recall_stay = eval_df['recall'][1]\n",
    "    recall_drop = eval_df['recall'][0]\n",
    "    recall_general = eval_df['recall'][3]\n",
    "    accuracy = eval_df['precision'][2]\n",
    "    class_imbalance = eval_df['support'][0] / eval_df['support'][1]\n",
    "    if algorithm == 'SVM_def' or algorithm == 'SVM_C':\n",
    "        rocauc = roc_auc_score(y_train, clf.decision_function(X_train))\n",
    "    else:\n",
    "        rocauc = roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "    cohen_kappa = cohen_kappa_score(y_train, y_pred)\n",
    "    precision_stay_train.append(precision_stay)\n",
    "    precision_drop_train.append(precision_drop)\n",
    "    recall_stay_train.append(recall_stay)\n",
    "    recall_drop_train.append(recall_drop)\n",
    "    recall_general_train.append(recall_general)\n",
    "    accuracy_train.append(accuracy)\n",
    "    rocauc_train.append(rocauc)\n",
    "    cohen_kappa_train.append(cohen_kappa)\n",
    "    #brier_loss_train.append(brier_loss)\n",
    "    class_imbalance_train.append(class_imbalance)\n",
    "    algorithm_train.append(algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45da4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking on test set\n",
    "precision_stay_test = []\n",
    "precision_drop_test = []\n",
    "recall_stay_test = []\n",
    "recall_drop_test = []\n",
    "recall_general_test = []\n",
    "accuracy_test = []\n",
    "rocauc_test = []\n",
    "cohen_kappa_test = []\n",
    "class_imbalance_test = []\n",
    "algorithm_test = []\n",
    "\n",
    "def add_to_final_table_test(X_test, y_test, algorithm, clf, names):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    eval_df = classification_report(y_test, y_pred, digits=3, target_names=names, output_dict=True)\n",
    "    eval_df = pd.DataFrame(eval_df).transpose()\n",
    "    \n",
    "    precision_stay = eval_df['precision'][1]\n",
    "    precision_drop = eval_df['precision'][0]\n",
    "    recall_stay = eval_df['recall'][1]\n",
    "    recall_drop = eval_df['recall'][0]\n",
    "    recall_general = eval_df['recall'][3]\n",
    "    accuracy = eval_df['precision'][2]\n",
    "    class_imbalance = eval_df['support'][0] / eval_df['support'][1]\n",
    "    if algorithm == 'SVM_def' or algorithm == 'SVM_C':\n",
    "        rocauc = roc_auc_score(y_test, clf.decision_function(X_test))\n",
    "    else:\n",
    "        rocauc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]) \n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    precision_stay_test.append(precision_stay)\n",
    "    precision_drop_test.append(precision_drop)\n",
    "    recall_stay_test.append(recall_stay)\n",
    "    recall_drop_test.append(recall_drop)\n",
    "    recall_general_test.append(recall_general)\n",
    "    accuracy_test.append(accuracy)\n",
    "    rocauc_test.append(rocauc)\n",
    "    cohen_kappa_test.append(cohen_kappa)\n",
    "    class_imbalance_test.append(class_imbalance)\n",
    "    algorithm_test.append(algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c749b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutting unnessesary leaves\n",
    "# https://stackoverflow.com/questions/51397109/prune-unnecessary-leaves-in-sklearn-decisiontreeclassifier\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "def is_leaf(inner_tree, index):\n",
    "    # Check whether node is leaf node\n",
    "    return (inner_tree.children_left[index] == TREE_LEAF and \n",
    "            inner_tree.children_right[index] == TREE_LEAF)\n",
    "\n",
    "def prune_index(inner_tree, decisions, index=0):\n",
    "    # Start pruning from the bottom - if we start from the top, we might miss\n",
    "    # nodes that become leaves during pruning.\n",
    "    # Do not use this directly - use prune_duplicate_leaves instead.\n",
    "    if not is_leaf(inner_tree, inner_tree.children_left[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_left[index])\n",
    "    if not is_leaf(inner_tree, inner_tree.children_right[index]):\n",
    "        prune_index(inner_tree, decisions, inner_tree.children_right[index])\n",
    "\n",
    "    # Prune children if both children are leaves now and make the same decision:     \n",
    "    if (is_leaf(inner_tree, inner_tree.children_left[index]) and\n",
    "        is_leaf(inner_tree, inner_tree.children_right[index]) and\n",
    "        (decisions[index] == decisions[inner_tree.children_left[index]]) and \n",
    "        (decisions[index] == decisions[inner_tree.children_right[index]])):\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "\n",
    "def prune_duplicate_leaves(mdl):\n",
    "    decisions = mdl.tree_.value.argmax(axis=2).flatten().tolist() # Decision for each node\n",
    "    prune_index(mdl.tree_, decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f50c8",
   "metadata": {},
   "source": [
    "defining all the models with default and tuned parameters, adding results to final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d80394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nensemble models are omitted due to obtaining similar results as other models with longer waiting times\\n\\n#########################################################\\n# XGBOOST ###############################################\\n#########################################################\\n# default parameters\\ndef XGBoost_default(X_train, y_train, X_test, y_test, names):    \\n        \\n    model = xgboost.XGBClassifier(verbosity=0)\\n    y_pred = cross_val_predict(estimator=model, X=X_train, y=y_train, cv=5)\\n    XGBoost_to_final_table(y_pred, y_train, names)\\n\\n# randomized search\\ndef XGBoost_RS(X_train, y_train, X_test, y_test, names):\\n    model = xgboost.XGBClassifier(verbosity=0)\\n    param = {\\n     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30] ,\\n     \"max_depth\"        : [3, 4, 5, 6, 10, 50],\\n     \"min_child_weight\" : [1, 3, 5, 7],\\n     \"gamma\"            : [0.0, 0.1, 0.2, 0.3, 0.4],\\n     \"colsample_bytree\" : [0.3, 0.4, 0.5, 0.7],\\n     \\'verbosity\\': [0]\\n     }\\n    XGB_random = RandomizedSearchCV(estimator = model, \\n                           param_distributions = param, n_iter = 50, \\n                           cv = 3)\\n    XGB_random.fit(X_train, y_train)\\n    y_pred = cross_val_predict(estimator=XGB_random, X=X_train, y=y_train, cv=5)\\n    XGBoost_to_final_table(y_pred, y_train, names)\\n    \\n#########################################################\\n# RANDOM FOREST #########################################\\n#########################################################\\n\\n# default parameters\\ndef RF_default(X_train, y_train, names):    \\n    \\n    clf = RandomForestClassifier()\\n    clf.fit(X_train, y_train)\\n    add_to_final_table(X_train, y_train, \\'RF_default\\', clf, names)\\n    \\n# randomized search\\ndef RF_random_search(X_train, y_train, names): \\n    \\n    clf = RandomForestClassifier()\\n    random_grid = {\\'bootstrap\\': [True, False],\\n     \\'max_depth\\': [50, 100, 200],\\n     \\'min_samples_leaf\\': [5, 10, 20],\\n     \\'min_samples_split\\': [20, 40],\\n     \\'n_estimators\\': [500, 2000, 4000]}\\n    clf = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\\n    # Fit the random search model\\n    clf.fit(X_train, y_train)\\n    print(\\'Random_forest_best: \\', clf.best_params_)\\n    add_to_final_table(X_train, y_train, \\'RF_RS\\', clf, names)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################################\n",
    "# DECISION TREE #########################################\n",
    "#########################################################\n",
    "\n",
    "def decision_tree(X_train, y_train, X_test, y_test, names):\n",
    "    weight_1 = Counter(y_train)[0]/y_train.shape[0]*0.9\n",
    "    weight_0 = 1 - weight_1\n",
    "    tree_params = {'min_samples_leaf': [20, 50, 100, 250]}\n",
    "    clf = GridSearchCV(DecisionTreeClassifier(max_depth=4, class_weight={\n",
    "                       0: weight_0, 1: weight_1}), tree_params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_estimator_)\n",
    "    prune_duplicate_leaves(clf.best_estimator_)\n",
    "\n",
    "    add_to_final_table(X_train, y_train, 'DT', clf, names)\n",
    "    add_to_final_table_test(X_test, y_test, 'DT', clf, names)\n",
    "    print(' ')\n",
    "\n",
    "    X_train.feature_names = X_train.columns\n",
    "    plt.figure(figsize=(45, 10))\n",
    "    _ = tree.plot_tree(clf.best_estimator_, filled=True,\n",
    "                       feature_names=X_train.feature_names, fontsize=12, precision=7, class_names=names)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# SVM (LinearSVC) #######################################\n",
    "#########################################################\n",
    "\n",
    "def SVM_default(X_train, y_train, X_test, y_test, names):\n",
    "    weight_1 = Counter(y_train)[0]/y_train.shape[0]*0.9\n",
    "    weight_0 = 1 - weight_1\n",
    "    clf = LinearSVC(max_iter=10000, class_weight={0: weight_0, 1: weight_1})\n",
    "    clf.fit(X_train, y_train)\n",
    "    add_to_final_table(X_train, y_train, 'SVM_def', clf, names)\n",
    "    add_to_final_table_test(X_test, y_test, 'SVM_def', clf, names)\n",
    "\n",
    "#########################################################\n",
    "# RBF SVM ###############################################\n",
    "#########################################################\n",
    "\n",
    "def RBF_SVM(X_train, y_train, X_test, y_test, names):\n",
    "    weight_1 = Counter(y_train)[0]/y_train.shape[0]*0.9\n",
    "    weight_0 = 1 - weight_1\n",
    "    clf = SVC(probability=True, class_weight={0: weight_0, 1: weight_1})\n",
    "    clf.fit(X_train, y_train)\n",
    "    add_to_final_table(X_train, y_train, 'RBF_SVM', clf, names)\n",
    "    add_to_final_table_test(X_test, y_test, 'RBF_SVM', clf, names)\n",
    "\n",
    "#########################################################\n",
    "# LOGISTIC REGRESSION ###################################\n",
    "#########################################################\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_test, y_test, names):\n",
    "    weight_1 = Counter(y_train)[0]/y_train.shape[0]*0.9\n",
    "    weight_0 = 1 - weight_1\n",
    "    clf = LogisticRegressionCV(max_iter=10000, class_weight={\n",
    "                               0: weight_0, 1: weight_1}).fit(X_train, y_train)\n",
    "    add_to_final_table(X_train, y_train, 'LR', clf, names)\n",
    "    add_to_final_table_test(X_test, y_test, 'LR', clf, names)\n",
    "    feature_coef = pd.DataFrame(clf.coef_)\n",
    "    feature_coef.columns = X_train.columns\n",
    "    feature_coef.index = ['coef']\n",
    "    feature_coef = feature_coef.T\n",
    "    feature_coef_abs = abs(feature_coef['coef'])\n",
    "    feature_coef_abs = feature_coef_abs.sort_values(ascending=False)\n",
    "    feature_coef_abs_10 = feature_coef_abs[:10]\n",
    "    most_important_features = feature_coef_abs_10.index\n",
    "\n",
    "    engin_features = X_train.loc[:, X_train.columns.isin(['voting_age_awareness_w1',\n",
    "                                                          'KNOWLEDGE_PARLIAMENTARY_THRESHOLD_w1',\n",
    "                                                          'know_politicians_ratio',\n",
    "                                                          'whether_dropped_before',\n",
    "                                                          'lr_placement_correct',\n",
    "                                                          'timeOfResponding',\n",
    "                                                          'weekendResponse',\n",
    "                                                          'whether_dropped_before',\n",
    "                                                          'inconsistency',\n",
    "                                                          'bad_quality',\n",
    "                                                          'weekend',\n",
    "                                                          'know_politicians_ratio',\n",
    "                                                          'same_agree_resp',\n",
    "                                                          'political_interest', \n",
    "                                                          'dont_know_percentage_mean',\n",
    "                                                          'days_to_respond'])].columns\n",
    "\n",
    "    def show_important(features, title):\n",
    "        \"\"\"filters the selected features with highest coefficients, adds title to the table\"\"\"\n",
    "        df = feature_coef[feature_coef.index.isin(features)]\n",
    "        df = df.style.set_table_attributes(\n",
    "            \"style='display:inline'\").set_caption(title)\n",
    "        return df\n",
    "\n",
    "    df_m = show_important(most_important_features,\n",
    "                          'The most important features and its coefficients obtained by logistic regression')\n",
    "    df_b = show_important(engin_features, 'Engineered features coefficients')\n",
    "    # displays one table on the left of another\n",
    "    display_html(df_m._repr_html_()+df_b._repr_html_(), raw=True)\n",
    "\n",
    "# ensemble models are omitted due to obtaining similar results as other models with longer waiting times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2093faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_train(X_train, X_test):\n",
    "    cols = X_train.columns\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train.columns = cols\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test.columns = cols\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15bd23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_all_models(X_train, y_train, X_test, y_test, names):\n",
    "    decision_tree(X_train, y_train, X_test, y_test, names)\n",
    "    X_train, X_test = scale_train(X_train, X_test)\n",
    "    SVM_default(X_train, y_train, X_test, y_test, names)\n",
    "    RBF_SVM(X_train, y_train, X_test, y_test, names)\n",
    "    logistic_regression(X_train, y_train, X_test, y_test, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f19070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Analysis_functions.ipynb to script\n",
      "[NbConvertApp] Writing 17922 bytes to Analysis_functions.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Analysis_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e92c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
